---
title: "Appendix S1 Bias in transformation as a function of the mean and variance of a sample"
author: "Bob Douma & James Weedon"
date: "18 February 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align="center")
```

## Bias resulting from  data transformations

The aim of this appendix is to illustrate the bias that can arise due to data transformations. We refer to the main text for a more detailed discussion where we argued that:

1. transformations will be most biased in the parts where the function is most non-linear and that
2. with increasing varation in the sample, the bias gets larger because the observations in the sample are spread over a larger part of the non-linear curve.

To illustrate these points we perform simulations comparing estimates based on samples taken from different distributions, and subject to different estimation procedures.
We generate samples from both a beta distribution and a logitnormal distribution with known means and variances.
Next, we estimate the mean and variance of each sample by either fitting a beta distribution by maximum likelihood, or by computing mean and standard deviation after logit transforming the sample.
The difference between the true mean and the mean estimated from the sample, averaged over many samples, is taken as an estimate of the bias of each procedure. 

The results of the simulations are summarized in Figure 1 and Figure 2 below (R code follows). These figures are based on 500 simulations and show the difference in the estimated mean and the mean which was used to generate the data, for the beta distribution and logit normal distribution respectively.
We conclude that the logit transformation is more biased towards the extremes (closer to 0 and 1) and that it increases with increasing variance for both the beta distribution and the logit-normal distributionm, while the bias in the beta regression is substantially less.

```{r fig1, echo=FALSE, out.width='50%',fig.cap="Bias in mean for estimation using logit-transformation (solid lines) and beta regression (dashed lines) when data is generated with a beta distribution" }
knitr::include_graphics('./bias_inmean_beta.png')
```

```{r fig2, echo=FALSE, out.width='50%',fig.cap="Bias in mean for estimation using logit-transformation (solid lines) and beta regression (dashed lines) when data is generated with a logit-normal distribution"}
knitr::include_graphics('./bias_inmean_logitnorm.png')
```

\newpage

# R code for simulations


## Loading libraries
```{r, eval=T}
library(betareg) # includes beta distribution
library(boot) # includes logit and inverse logit transformation
library(ggplot2) # for plotting
library(nloptr) # optimiser package
library(utils) # for progressbar
library(reshape2) # for melting dataframes
```

## User defined functions
### Estimating `shape1` and `shape2` of beta distribution from mean and variance/precision
```{r,eval=T}
# function to estimate alpha and beta from mu, variance or precision
estaBeta_A_B <- function(mu,var=NULL,precision=NULL){
  if (!is.null(var)){
    alpha <- ((1-mu)/var-1/mu)*mu^2
    beta <- alpha * (1/mu-1)
  } else if (!is.null(precision)){
    alpha <- mu*precision
    beta <- -mu*precision + precision
  }
  return(params=list(alpha=alpha,beta=beta))
}


# function to transform fraction with 0 and 1 to <1 and >0
transform01 <- function(x){
  (x * (length(x) -  1) + 0.5)/(length(x))
}
# function to backtransform transform01
backtransform.est <- function(x,n){
  y <- (x*n-0.5)/(n-1)
  return(y)
}

```
### Estimate the parameters of the logitnormal

The probability distribution function logit normal distribution is defined as:
$\frac{1}{\sigma \sqrt(2\pi)} \frac{1}{x(1-x)}e^{-\frac{(logit(x)-\mu)^2}{2\sigma^2}}$
with $logit(x) = log(\frac{x}{1-x})$


There is no closed-form solution of the parameters of the logitnormal distribution, i.e. one cannot calculate analytically which $\mu$ and $\sigma$ in the logit normal lead to a given mean and variance in the sample. For this reason the parameters were found by using an optimisation approach. The approach has three steps: 1) a cost function that calculate the difference between the mean and variance in a sample versus what is generated by the logit normal given a set of parameter values; 2) an optimisation routine that varies the parameter values of the logit normal and return the values for which the difference between the observed mean and variance are as close as possible to one that were generated by the parameters in the logit normal; 3) a check whether the parameters found in the optimisation approach indeed lead to the desired mean and variance of the sample.

```{r,eval=F}
#1 cost function
est.par <- function(par,x){ # x is the sample that is tested
  sim <- inv.logit(rnorm(1000000,par[1],par[2])) # generate sims from logit normal
  vars <- (x[,2]-var(sim))^2/x[,2] # calculate variance
  means <- (x[,1]-mean(sim))^2/x[,1] # calculate mean
  sum <- vars+means # minimize the relative SSQ of both variance and mean
  return(sum)
}

# 2 Function that calls optimizer function to match 
init <- read.csv("init.csv") # for initial values of opimiser
comb$sdl <- NA
comb$meanl <- NA
opts <- list("algorithm"="NLOPT_GN_DIRECT",stopval=0.0001,maxeval=5000)
for (i in 1:nrow(comb)){
  par <- c((init[i,4]),init[i,3])
  # global search for optimum  
  opt <- nloptr(x0=par,eval_f=est.par,lb=c(-10,0),ub=c(10,5),opts=opts,x=comb[i,])
  par <- opt$solution
  # local search for optimum
  opts <- list("algorithm"="NLOPT_LN_SBPLX",xtol_rel=1e-8,maxeval=5000)
  opt <- nloptr(x0=par,eval_f=est.par,lb=c(-10,0),ub=c(10,5),opts=opts,x=comb[i,]) 
  comb$sdl[i] <- opt$solution[2]
  comb$meanl[i] <- opt$solution[1]
  comb$obj[i] <- opt$objective
  comb$message[i] <- opt$iterations
}

# 3. Test whether estimated parameters indeed lead to the desired mean and variance
# as specified in the dataframe `comb`.
for (i in 1:nrow(comb)){
  # generate sims from logit normal distribution
  sim <- inv.logit(rnorm(10000000,comb[i,4],sd=comb[i,3])) 
  comb$avgl[i] <- mean(sim)
  print(i)
}
```


Now we define a function to assess the bias for logit transformed data. In addition, we estimate the random variates through a beta-regression model.

```{r, eval=T}
# function to return when betaregression model fails
beta1 <- function(x){return(NA)} 
# function to return "failure" in case model fitted fails
warnings <- function(x){return(c(NA,"failure"))} #

# function to calculate bias of a sample
bias <- function(x,logitnorm=F,n=n){ 
  # when using logitnorm = T samples are generated from a logit-normal distribution
  if (logitnorm) { 
    # if data is generated by a logit normal distribution
    # generate sims from logit normal distribution  
    sim <- inv.logit(rnorm(n,mean=(comb[x,4]),sd=comb[x,3])) 
  } else { # data is generated from a beta distribution
    # take parameter combination to estimate a and b for beta distribution
    pars <- estaBeta_A_B(mu=comb[x,1],var=comb[x,2])  
    sim <- rbeta(n,pars$alpha,pars$beta) # generate sims for beta distribution
  }
  if (length(unique(sim))> 1){ 
    # parameters can only be assessed when the simulated values are slightly dissimilar
    if (max(sim)==1 | min(sim)==0){ 
      # if data contains zeros and ones, a linear transformation is applied
      sim <- transform01(sim) # transform data
      # logit transformation
      meanlog <- inv.logit(mean(logit(sim)))    
      meanlog <- backtransform.est(meanlog,length(sim)) # model estimates are backtransformed
      # beta regression model
      beta <- tryCatch(inv.logit(coef(betareg(sim~1|1)))[1],error=beta1,warning=warnings)
      beta <- backtransform.est(beta,length(sim))  # model estimates are backtransformed
      
    } else {
      # logit transformation
      meanlog <- inv.logit(mean(logit(sim)))    
      # beta regression model
      beta <- tryCatch(inv.logit(coef(betareg(sim~1|1)))[1],error=beta1,warning=warnings)
      
    }
  } else { # return NA for beta otherwise
    beta <- NA
  }
  out <- c(x=x,meanp=comb[x,1],varp=comb[x,2],means=mean(sim),vars = var(sim),
          meanlog = meanlog,meanbeta=beta[1],fail=beta[2])  # store output
  return(out)
}
```

## Set simulation characteristics
A range of different means and variances are specified and combined factorially. These combinations are stored in dataframe `comb`. Next, we specify how many samples are drawn per replication (`n`) and how often this is repeated (`sim`). 


```{r,eval=T}
meanprop <- seq(0.05,0.95,length.out=10) # means proportions to be generated
meanprop <- sort(c(meanprop,0.08,0.9,0.5)) # add some other interesting means
var <- 10^seq(log10(0.0005),log10(0.03),length.out=6) # sequence of variances
comb <- expand.grid(meanprop,var) # get all combinations of mean and variance
n <- 20 # number of values generated per iteration
sim <- 2 #number of sims per combination of mean and var. Set to larger number for serious work
```

## Run simulation
For all combinations of the mean and variance the mean is estimated through a logit transformation and a beta-regression (without covariate) and stored in a list called `out`. If you want to draw samples from a logitnormal distribution, set `logitnorm=TRUE`. In addition, run the code above to get the parameters for the logitnormal distribution. This may take a while.

```{r,eval=T,echo=F, cache=TRUE}

out <- lapply(rep(1:78,each=sim),bias,logitnorm=F,n=n) # slow calculation!!
```

## Analysing output
For each mean and variance, we calculate the mean bias (i.e. difference of the estimated mean and the true mean). Since the logitnormal distribution has no closed form solution with respect to the mean and variance, we estimated this through an optimisation approach (see below). Hence the bias is assessed with respect to the mean and variance at proportion scale that is close to the imposed mean and variance (`avgl`), but not exactly similar.
```{r,eval=T}
out <- as.data.frame(do.call(rbind,out)) # merge list to dataframe
out <- out[is.na(out$fail.NA),] # remove failures 
out <- out[out$meanlog!="NaN",] # remove NAs if any
#write.csv(out,"sims_var_mean_logitnorm.csv") # store results if wanted
#out = read.csv(sims_var_mean_logitnorm.csv) # read results if wanted
logitnorm <- F
if (logitnorm){
  #calculate difference between mean and estimated means of logit transformation
  out$diff <- (out$meanlog - -out$meanp) 
  # calculate difference between mean and estimated means of beta
  out$diffbeta <- (out$meanbeta - out$meanp) 
} else {
# in case data is generated from logitnormal; 
#their parameters slightly deviate from the desired values
  out$avgl <- comb[match(paste(out[,2],out[,3],sep=""),paste(comb[,1],comb[,2],sep="")),"avgl"] 
  # adds the   mean proportion that is generated from the logit normal
  
  # difference between estimated and mean generated by logitnormal
  out$diff <- (out$meanlog - out$avgl) 
  
  # difference between estimated and mean generated by logitnormal
  out$diffbeta <- (out$meanbeta - out$avgl)
}


# calculate average across replications
outsum <- melt(tapply(out$diff,list("means"=out$meanp,"vars"=out$varp),mean))

outsum$diffbeta <- melt(tapply(out$diffbeta,list("means"=out$meanp,"vars"=out$varp),mean))[,3]

outsum <- na.omit(outsum)
outsum1 <- melt(outsum,id.var=c("means","vars"))
```

## Plot the bias as function of the mean and variance
```{r,eval=F}
ggplot(data=outsum1)+
  geom_line(aes(x=means,y=value,colour=vars,linetype=variable))+
  #geom_line(aes(x=means,y=diffbeta,colour=vars),linetype=2)+
  ylab("Bias in estimated mean")+
  xlab("mean")+
  scale_colour_discrete(name="variance")+
  geom_hline(yintercept=0)+
  scale_linetype_discrete(name="regression",labels=c("logit trans","beta"))+
  theme(axis.text.x=element_text(size=12), axis.text.y=element_text(size=12),
        axis.title.y=element_text(size=12),axis.title.x=element_text(size=12),
        panel.background = element_rect(fill = "white", 
        colour = NA), panel.border = element_rect(fill = NA, 
        colour = "grey50"), panel.grid.major = element_line(colour = "grey90", 
        size = 0.2), panel.grid.minor = element_line(colour = "grey98", 
        size = 0.5), strip.background = element_rect(fill = "grey80", 
        colour = "grey50"))

```



